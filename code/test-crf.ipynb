{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test-crf.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yaa9cAjBnBjJ"},"source":["**Test the BioNER model on N2C2 2018 Track 2 dataset using trained Clinical-BERT. Save to /output**"]},{"cell_type":"markdown","metadata":{"id":"a83I0j3JgS8Y"},"source":["**Data versions**\n","- v1 = Sentence-level input + Overlap filtering + max seq len(~192)\n","- **(BEST)v2** = v1 + reduced max seq length to ~100\n","\n","**Model versions**\n","\n","BERT-LR\n","- v1 = Bio_Discharge_Summary_BERT(data=v1)\n","- v2 = Bio_Discharge_Summary_BERT(data=v1) trained with weights, scheduler\n","- v3 = reduced max seq length 128 , 150 epoch, 16 batch, 2e-5 lr(val= 70)\n","- v4 = reduced max seq length 128(272) , 150 epoch, 32 batch, 3e-5 lr, dropout = 0.1 (val= 70, test(strict)= 73, test(lenient)= 84)\n","\n","Note- max seq length ~350 (Stopped as the f1 was 0.40 at 80th epoch due to lost info via clipping)\n","\n","BERT-CRF\n","- **(BEST)v5** = BERT-CRF with max seq length 128(272) (data=v2) (val= 84, test(strict)= 85, test(lenient)= 90)\n","- v6 = BERT-CRF with max seq length 128(384) (data=v2)\n"]},{"cell_type":"markdown","metadata":{"id":"27nM9EWCvp7B"},"source":["# Initialize Parameters"]},{"cell_type":"code","metadata":{"id":"W4qZcOFazKU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771405411,"user_tz":420,"elapsed":439,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"7c6190a0-a9fe-48ad-ddec-26c2e95b7300"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2qdBqWItzNQF"},"source":["DATA_VER = \"v2\"\n","MODEL_VER = \"v5\"\n","PARENT_DIR = \"/content/gdrive/My Drive/projects/biomedical_ner\"\n","TEST_DIR = PARENT_DIR + \"/data/\" + DATA_VER + \"/test\"\n","MODEL_DIR = PARENT_DIR + \"/model/\" + MODEL_VER\n","OUTPUT_DIR = PARENT_DIR + \"/output/\" + MODEL_VER\n","\n","MODEL_PATH = MODEL_DIR + \"/pytorch_model.pt\"\n","VOCAB_PATH = MODEL_DIR + \"/vocab.txt\"\n","PREDICTIONS_PATH = OUTPUT_DIR + \"/predictions.csv\"\n","REPORT_PATH = OUTPUT_DIR + \"/result.txt\"\n","CONFUSION_MATRIX_PATH = OUTPUT_DIR + \"/confusion_matrix.csv\"\n","SCORES_PATH = OUTPUT_DIR + \"/scores.csv\"\n","\n","BERT_VARIANT = \"emilyalsentzer/Bio_Discharge_Summary_BERT\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Wls5pvb0Bv1"},"source":["import os\n","if not os.path.exists(OUTPUT_DIR):\n","  os.makedirs(OUTPUT_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGSTO1mSeXJQ"},"source":["batch_size = 16\n","max_len = 272 # tried 384\n","pad_label = \"X\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hw-Ttwsv0JLQ"},"source":["# Requirements Installation"]},{"cell_type":"code","metadata":{"id":"iECmqSjR0L1Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771410611,"user_tz":420,"elapsed":5611,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"1cafd36b-69c9-4ee8-95bf-c701b51feef5"},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","!pip install seqeval\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B9-LtJ1i0H_c"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"O9ctAbjWe0pg"},"source":["import pandas as pd\n","import math\n","import numpy as np\n","from seqeval.metrics import classification_report,accuracy_score,f1_score\n","import torch.nn.functional as F\n","import torch\n","import os\n","from tqdm import tqdm,trange\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, SequentialSampler, Dataset, ConcatDataset\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import AutoModel, BertTokenizer, AutoConfig, AutoModelForTokenClassification, AutoTokenizer, BertForTokenClassification\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zCu8LNaie8Ku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771413670,"user_tz":420,"elapsed":8654,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"a8305cae-a27e-4d03-abdf-75dbfa9eab97"},"source":["# Check library version\n","!pip list | grep -E 'transformers|torch|Keras'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Keras                         2.4.3          \n","Keras-Preprocessing           1.1.2          \n","torch                         1.7.0+cu101    \n","torchsummary                  1.5.1          \n","torchtext                     0.3.1          \n","torchvision                   0.8.1+cu101    \n","transformers                  4.0.0          \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IkHQ-u31fcQN"},"source":["# Setup Mapping"]},{"cell_type":"code","metadata":{"id":"qKttzOjFfekC"},"source":["tag2idx = {'B-Drug': 0,\n","          'I-Drug': 1,\n","          'B-Reason': 2,\n","          'I-Reason': 3,\n","          'B-ADE': 4,\n","          'I-ADE': 5,\n","          'O': 6,\n","          'X': 7,\n","          '[CLS]': 8,\n","          '[SEP]': 9\n","          }\n","tag2name = {tag2idx[key] : key for key in tag2idx.keys()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aj9vz8Oyflxr"},"source":["# Setup GPU"]},{"cell_type":"code","metadata":{"id":"WyGVs6_Qfmvn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771413672,"user_tz":420,"elapsed":8645,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"a74d1b98-2a8d-484b-b1a6-32be8d133537"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","n_gpu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"NYMzH2CF0W0i"},"source":["# Prepare Data"]},{"cell_type":"code","metadata":{"id":"_oLevWWwfrcY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771413672,"user_tz":420,"elapsed":8638,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"98bbcdac-ed8c-472c-f3ca-64b8ee6aad7b"},"source":["!ls '$TEST_DIR' | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["202\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ub3QMZRZ0Ywy"},"source":["class ClinicalDataset(Dataset):\n","    def __init__(self, file, path, max_seq_len, tag2idx, tokenizer):\n","        self.max_seq_len = max_seq_len;\n","        self.path = os.path.join(path, file)\n","        self.df = pd.read_csv(self.path, names=['patientID', 'sentenceID', 'token', 'tag'], keep_default_na=False)\n","        self.tag2idx = tag2idx\n","        self.tokenizer = tokenizer\n","        # Convert Tokens to indices\n","        self.prepare_data()\n","\n","    def prepare_data(self):\n","        sentences, labels = self.get_sentences(self.df)\n","        tokenized_texts, word_piece_labels = self.tokenize_text(sentences, labels)\n","\n","        # Make text token into id\n","        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                                  maxlen=self.max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","        # Make label into id, pad with \"O\" meaning others/wrong\n","        tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n","                             maxlen=self.max_seq_len, value=self.tag2idx[pad_label],\n","                             padding=\"post\", dtype=\"long\", truncating=\"post\")\n","        \n","        input_text = pad_sequences([txt for txt in tokenized_texts],\n","                                  maxlen=self.max_seq_len, value = \"[PAD]\", \n","                                   padding=\"post\", dtype=object, truncating=\"post\")\n","        \n","        y_text.extend(input_text)\n","\n","        # For fine tune of predict, with token mask is 1,pad token is 0\n","        attention_masks = [[int(i > 0) for i in ii] for ii in input_ids]\n","        \n","        self.Sentences = torch.tensor(input_ids)\n","        self.label_data = torch.tensor(tags)\n","        self.attention_masks = torch.tensor(attention_masks)\n","\n","    def get_sentences(self, data):\n","        agg_func = lambda s: [(w, t) for w, t in zip(s[\"token\"].values.tolist(), s[\"tag\"].values.tolist())]\n","        grouped = data.groupby(\"sentenceID\").apply(agg_func)\n","        tokenstags = [s for s in grouped]\n","        sentences = [[s[0] for s in sent] for sent in tokenstags]\n","        labels = [[s[1] for s in sent] for sent in tokenstags]\n","        return sentences, labels\n","\n","    def tokenize_text(self, sentences, labels):\n","        tokenized_texts = []\n","        word_piece_labels = []\n","        i_inc = 0\n","        for word_list, label in (zip(sentences,labels)):\n","            temp_label = []\n","            temp_token = []\n","\n","            # Add [CLS] at the front\n","            temp_label.append('[CLS]')\n","            temp_token.append('[CLS]')\n","\n","            for word,lab in zip(word_list,label):\n","                token_list = self.tokenizer.tokenize(word)\n","                for m,token in enumerate(token_list):\n","                    temp_token.append(token)\n","                    if lab.startswith('B'):\n","                        if m==0:\n","                            temp_label.append(lab)\n","                        else:\n","                            temp_label.append('I-'+lab.split('-')[1])\n","                    else:\n","                        temp_label.append(lab)\n","\n","            # Add [SEP] at the end\n","            temp_token.append('[SEP]')\n","            temp_label.append('[SEP]')\n","\n","            tokenized_texts.append(temp_token)\n","            word_piece_labels.append(temp_label)\n","\n","        return tokenized_texts, word_piece_labels\n","\n","    def __len__(self):\n","        return len(self.Sentences)\n","\n","    def __getitem__(self, idx):\n","        return self.Sentences[idx], self.attention_masks[idx], self.label_data[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-r5b_vif8tu"},"source":["# Tokenizer\n","tokenizer = BertTokenizer(vocab_file=VOCAB_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YKD4oMVf_Bn"},"source":["# TEST DATASET\n","test_datasets = []\n","y_text = []\n","for doc in os.listdir(TEST_DIR):\n","    test_datasets.append(ClinicalDataset(doc, TEST_DIR, max_len, tag2idx, tokenizer))\n","\n","# concatenate CSV data\n","test_dataset = ConcatDataset(test_datasets)\n","\n","test_sampler = SequentialSampler(test_dataset)\n","\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size) # drop_last=True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_HnzTOam5AA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771446370,"user_tz":420,"elapsed":41318,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"304cf169-3678-4d2b-8fc6-26c788487bd2"},"source":["print(f'Dataset length - {len(test_dataset)}, Dataloader length - {len(test_dataloader)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset length - 6115, Dataloader length - 383\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5PTT0PLZ0Z9B"},"source":["# Load Model\n","\n","[ref](https://github.com/Louis-udm/NER-BERT-CRF/blob/master/NER_BERT_CRF.py)"]},{"cell_type":"code","metadata":{"id":"EztNezoe4zAr"},"source":["def log_sum_exp_1vec(vec):  # shape(1,m)\n","    max_score = vec[0, np.argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n","\n","def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n","    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n","\n","def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n","    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n","\n","\n","class BERT_CRF_NER(nn.Module):\n","\n","    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n","        super(BERT_CRF_NER, self).__init__()\n","        self.hidden_size = 768\n","        self.start_label_id = start_label_id\n","        self.stop_label_id = stop_label_id\n","        self.num_labels = num_labels\n","        # self.max_seq_length = max_seq_length\n","        self.batch_size = batch_size\n","        self.device=device\n","\n","        # use pretrainded BertModel \n","        self.bert = bert_model\n","        self.dropout = torch.nn.Dropout(0.2)\n","        # Maps the output of the bert into label space.\n","        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.num_labels, self.num_labels))\n","\n","        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n","        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n","        # so this enforcement is likely unimportant)\n","        self.transitions.data[start_label_id, :] = -10000\n","        self.transitions.data[:, stop_label_id] = -10000\n","\n","        nn.init.xavier_uniform_(self.hidden2label.weight)\n","        nn.init.constant_(self.hidden2label.bias, 0.0)\n","        # self.apply(self.init_bert_weights)\n","\n","    def init_bert_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)): \n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def _forward_alg(self, feats):\n","        '''\n","        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n","        '''\n","        \n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","        \n","        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n","        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n","        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n","        # self.start_label has all of the score. it is log,0 is p=1\n","        log_alpha[:, 0, self.start_label_id] = 0\n","        \n","        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n","        # feats is the probability of emission, feat.shape=(1,tag_size)\n","        for t in range(1, T):\n","            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n","\n","        # log_prob of all barX\n","        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n","        return log_prob_all_barX\n","\n","    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n","        '''\n","        sentances -> word embedding -> lstm -> MLP -> feats\n","        '''\n","        bert_seq_out, last_hidden = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask)\n","        bert_seq_out = self.dropout(bert_seq_out)\n","        bert_feats = self.hidden2label(bert_seq_out)\n","        return bert_feats\n","\n","    def _score_sentence(self, feats, label_ids):\n","        ''' \n","        Gives the score of a provided label sequence\n","        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n","        '''\n","        \n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","\n","        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n","        batch_transitions = batch_transitions.flatten(1)\n","\n","        score = torch.zeros((feats.shape[0],1)).to(device)\n","        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n","        for t in range(1, T):\n","            score = score + \\\n","                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n","                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        '''\n","        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n","        '''\n","        \n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","\n","        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n","\n","        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n","        log_delta[:, 0, self.start_label_id] = 0\n","        \n","        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n","        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n","        for t in range(1, T):\n","            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n","            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n","            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n","            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n","            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n","            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n","\n","        # trace back\n","        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n","\n","        # max p(z1:t,all_x|theta)\n","        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n","\n","        for t in range(T-2, -1, -1):\n","            # choose the state of z_t according the state choosed of z_t+1.\n","            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n","\n","        return max_logLL_allz_allx, path\n","\n","    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n","        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n","        forward_score = self._forward_alg(bert_feats)\n","        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n","        gold_score = self._score_sentence(bert_feats, label_ids)\n","        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n","        return torch.mean(forward_score - gold_score)\n","\n","    # this forward is just for predict, not for train\n","    # dont confuse this with _forward_alg above.\n","    def forward(self, input_ids, segment_ids, input_mask):\n","        # Get the emission scores from the BiLSTM\n","        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n","\n","        # Find the best path, given the features.\n","        score, label_seq_ids = self._viterbi_decode(bert_feats)\n","        return score, label_seq_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SReVBLWtgKcf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771455627,"user_tz":420,"elapsed":50565,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"9779dfba-6de1-4815-9deb-04952e0960c2"},"source":["bert_model = AutoModel.from_pretrained(BERT_VARIANT, output_hidden_states=False, return_dict=False)\n","start_label_id = tag2idx[\"[CLS]\"]\n","stop_label_id = tag2idx[\"[SEP]\"]\n","\n","model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(tag2idx), max_len, batch_size, device)\n","\n","# LOAD TRAINED MODEL\n","checkpoint = torch.load(MODEL_PATH, map_location='cpu')\n","epoch = checkpoint['epoch']\n","valid_f1_prev = checkpoint['valid_f1']\n","pretrained_dict=checkpoint['model_state']\n","net_state_dict = model.state_dict()\n","pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n","net_state_dict.update(pretrained_dict_selected)\n","model.load_state_dict(net_state_dict)\n","\n","print('Loaded the pretrain  NER_BERT_CRF  model, epoch:',checkpoint['epoch'], 'valid f1:', checkpoint['valid_f1'])\n","model.cuda();"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded the pretrain  NER_BERT_CRF  model, epoch: 13 valid f1: 0.8475551294343241\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGojUWFY4Zaf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771455629,"user_tz":420,"elapsed":50555,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"bfe083e3-7163-42f6-8aea-077c98ef5da3"},"source":["!ls '$MODEL_DIR'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pytorch_model.pt  vocab.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dewaogaU0qyz"},"source":["# Test Model"]},{"cell_type":"code","metadata":{"id":"a2fHBcv00vZm"},"source":["model.eval();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8JQ9zDq0w6C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771573916,"user_tz":420,"elapsed":168830,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"22bab944-e451-4f1d-bd88-aadb398a5a52"},"source":["y_true = []\n","y_pred = []\n","y_confidence = []\n","\n","probs = []\n","out = []\n","print(\"***** Running evaluation *****\")\n","print(\"  Num examples = {}\".format(len(test_dataset)))\n","print(\"  Batch size = {}\".format(batch_size))\n","for step, batch in enumerate(test_dataloader):\n","    batch = tuple(t.to(device) for t in batch)\n","    input_ids, input_mask, label_ids = batch\n","    \n","    with torch.no_grad():\n","        _, predicted_label_seq_ids = model(input_ids, None, input_mask)\n","        # For eval mode, the first result of outputs is logits\n","    \n","    # Model Confidence\n","    # logits_prob, _ = torch.max(F.softmax(logits, dim=2),dim=2)\n","    # logits_prob = logits_prob.detach().cpu().numpy()\n","\n","    # print(logits_prob)\n","    # logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n","    # logits = logits.detach().cpu().numpy()\n","    \n","    # Get NER true result\n","    label_ids = label_ids.to('cpu').numpy()\n","    \n","    # Only predict the real word, mark=0, will not calculate\n","    input_mask = input_mask.to('cpu').numpy()\n","\n","    predicted_label_ids = predicted_label_seq_ids.to('cpu').numpy()\n","    \n","    # Compare the valuable predict result\n","    for i,mask in enumerate(input_mask):\n","        # Ground truth\n","        temp_true = []\n","        # Prediction\n","        temp_pred = []\n","\n","        # temp_confidence = []\n","        for j, m in enumerate(mask):\n","            # Mark=0 (Label_ids = \"X\"), meaning its a pad word, dont compare\n","            if m:\n","                if tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\":\n","                    temp_true.append(tag2name[label_ids[i][j]])\n","                    temp_pred.append(tag2name[predicted_label_ids[i][j]])\n","                    # temp_confidence.append(logits_prob[i][j])\n","            else:\n","                break\n","        \n","        y_true.append(temp_true)\n","        y_pred.append(temp_pred)\n","        # y_confidence.append(temp_confidence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Running evaluation *****\n","  Num examples = 6115\n","  Batch size = 16\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YYX92Mxz04wQ"},"source":["# Predictions"]},{"cell_type":"code","metadata":{"id":"Z8QAaf__07T9","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606771574447,"user_tz":420,"elapsed":169354,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"0a146a42-dff9-448e-8f95-1a26aa572832"},"source":["result = pd.DataFrame()\n","result[\"actual\"] = y_true\n","result[\"predicted\"] = y_pred\n","# result[\"confidence\"] = y_confidence\n","\n","result.to_csv(PREDICTIONS_PATH, sep=\",\", encoding=\"utf-8\", index=False)\n","result.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>actual</th>\n","      <th>predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[O, O, O, O, B-Drug, I-Drug, I-Drug, I-Drug, O...</td>\n","      <td>[O, O, O, O, B-Drug, I-Drug, I-Drug, I-Drug, O...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, B-Reason, I-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              actual                                          predicted\n","0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n","1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n","2  [O, O, O, O, B-Drug, I-Drug, I-Drug, I-Drug, O...  [O, O, O, O, B-Drug, I-Drug, I-Drug, I-Drug, O...\n","3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [O, O, O, O, O, O, O, O, O, O, O, B-Reason, I-...\n","4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ..."]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"gsqO_PL41LG9"},"source":["# Analysis"]},{"cell_type":"code","metadata":{"id":"xrpuqpy2lcol"},"source":["# UTILS\n","y_true_total = [item for sublist in y_true for item in sublist]\n","y_pred_total = [item for sublist in y_pred for item in sublist]\n","\n","y_text = [[l_i for l_i in l if (l_i != \"[PAD]\" and l_i != \"[CLS]\" and l_i != \"[SEP]\")] for l in y_text]\n","y_text_total = [item for sublist in y_text for item in sublist]\n","# y_confidence_total = [item for sublist in y_confidence for item in sublist]\n","\n","\n","def get_cleaned_label(label: str):\n","    if \"-\" in label:\n","        return label.split(\"-\")[1]\n","    else:\n","        return label\n","\n","\n","interested_b = {\"B-Drug\", \"B-Reason\", \"B-ADE\"}\n","interested_i = {\"I-Drug\", \"I-Reason\", \"I-ADE\"}\n","n = len(y_true_total)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwTm-YCSqQqh"},"source":["**Classification Report**"]},{"cell_type":"code","metadata":{"id":"27pxI7343AL7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606772247203,"user_tz":420,"elapsed":12635,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"aee54bdc-dd30-495c-af11-422db40a77d6"},"source":["from seqeval.scheme import IOB2\n","\n","# Get acc , recall, F1 result report(strict)\n","report_strict = classification_report(y_true, y_pred, mode='strict', scheme=IOB2, digits=4)\n","\n","# Save the report into file\n","with open(REPORT_PATH, \"w\") as writer:\n","    print(\"***** Eval results(Strict) *****\")\n","    print(\"\\n%s\"%(report_strict))\n","    print(\"F1 score: %f\"%(f1_score(y_true, y_pred,zero_division=1, mode='strict', scheme=IOB2)))\n","    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n","    \n","    writer.write(\"F1 score(Strict):\\n\")\n","    writer.write(str(f1_score(y_true, y_pred, mode='strict', scheme=IOB2)))\n","    writer.write(\"\\n\\nAccuracy score:\\n\")\n","    writer.write(str(accuracy_score(y_true, y_pred)))\n","    writer.write(\"\\n\\n\")  \n","    writer.write(report_strict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Eval results(Strict) *****\n","\n","              precision    recall  f1-score   support\n","\n","         ADE     0.5214    0.3626    0.4277       604\n","        Drug     0.9228    0.9315    0.9271     10569\n","      Reason     0.6725    0.5836    0.6249      2519\n","\n","   micro avg     0.8689    0.8424    0.8554     13692\n","   macro avg     0.7056    0.6259    0.6599     13692\n","weighted avg     0.8590    0.8424    0.8495     13692\n","\n","F1 score: 0.855416\n","Accuracy score: 0.986052\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ujsjF7LeqWld"},"source":["**Confusion Matrix**"]},{"cell_type":"code","metadata":{"id":"lq5CxzLWMkd2","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1606771588067,"user_tz":420,"elapsed":182962,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"2aad1937-a4ab-4f52-d54d-193fa846ec02"},"source":["conf_mat = {\"Drug\": {\"Drug\": 0, \"Reason\": 0, \"ADE\": 0, \"O\": 0}, \n","            \"Reason\": {\"Drug\": 0, \"Reason\": 0, \"ADE\": 0, \"O\": 0}, \n","            \"ADE\": {\"Drug\": 0, \"Reason\": 0, \"ADE\": 0, \"O\": 0},\n","            \"O\": {\"Drug\": 0, \"Reason\": 0, \"ADE\": 0, \"O\": 0}}\n","\n","i = 0\n","while i < n:\n","  if y_true_total[i] in interested_b:\n","\n","    # If first label matches just mark TP and move ahead till the end of I- tags\n","    if get_cleaned_label(y_pred_total[i]) == get_cleaned_label(y_true_total[i]):\n","      conf_mat[get_cleaned_label(y_true_total[i])][get_cleaned_label(y_true_total[i])] += 1\n","      i += 1\n","      while i<n and y_true_total[i] in interested_i:\n","        i += 1\n","\n","    else:\n","      wrong_pred = get_cleaned_label(y_pred_total[i])\n","\n","      i += 1\n","      # start checking I- tags till they either exhaust or atleast cleaned label matches\n","      while i<n and y_true_total[i] in interested_i and get_cleaned_label(y_pred_total[i]) != get_cleaned_label(y_true_total[i]):\n","        # fetching what the wrongly predicted entity is\n","        if get_cleaned_label(y_pred_total[i]) in conf_mat: \n","          wrong_pred = get_cleaned_label(y_pred_total[i])\n","        i += 1\n","\n","      if i>n:\n","        # marking the false negative of true tag\n","        conf_mat[get_cleaned_label(y_true_total[i-1])][wrong_pred] += 1\n","        break\n","\n","      if y_true_total[i] not in interested_i:\n","        # marking the false negative of true tag\n","        conf_mat[get_cleaned_label(y_true_total[i-1])][wrong_pred] += 1\n","\n","      else:\n","        # lenient marking if atleast something matches and moving ahead till the end of I- tags\n","        conf_mat[get_cleaned_label(y_true_total[i])][get_cleaned_label(y_true_total[i])] += 1\n","        while i<n and y_true_total[i] in interested_i:\n","          i += 1\n","      \n","  else:\n","    if y_pred_total[i].startswith('B') or y_pred_total[i].startswith('O'):\n","      conf_mat[get_cleaned_label(y_true_total[i])][get_cleaned_label(y_pred_total[i])] += 1\n","    i += 1    \n","\n","\n","confusion_matrix = pd.DataFrame.from_dict(conf_mat, orient='index')\n","confusion_matrix.drop(\"O\", inplace=True)\n","confusion_matrix.to_csv(CONFUSION_MATRIX_PATH, sep=\",\", encoding=\"utf-8\")\n","\n","for key in conf_mat:\n","  print(key, conf_mat[key])\n","confusion_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drug {'Drug': 10155, 'Reason': 3, 'ADE': 0, 'O': 411}\n","Reason {'Drug': 11, 'Reason': 1732, 'ADE': 30, 'O': 746}\n","ADE {'Drug': 1, 'Reason': 33, 'ADE': 288, 'O': 282}\n","O {'Drug': 482, 'Reason': 491, 'ADE': 136, 'O': 736306}\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Drug</th>\n","      <th>Reason</th>\n","      <th>ADE</th>\n","      <th>O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Drug</th>\n","      <td>10155</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>411</td>\n","    </tr>\n","    <tr>\n","      <th>Reason</th>\n","      <td>11</td>\n","      <td>1732</td>\n","      <td>30</td>\n","      <td>746</td>\n","    </tr>\n","    <tr>\n","      <th>ADE</th>\n","      <td>1</td>\n","      <td>33</td>\n","      <td>288</td>\n","      <td>282</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Drug  Reason  ADE    O\n","Drug    10155       3    0  411\n","Reason     11    1732   30  746\n","ADE         1      33  288  282"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"huoZwow7R9wL"},"source":["**Entity Count**"]},{"cell_type":"code","metadata":{"id":"nguk-q6FR8uZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771588888,"user_tz":420,"elapsed":183777,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"871c5d43-a170-4e9b-9273-48a7b7212d1d"},"source":["def get_counts(conf_mat):\n","  counts = {\"Drug\": 0, \"Reason\": 0, \"ADE\": 0}\n","  for key in conf_mat:\n","    count = 0\n","    for k in conf_mat[key]:\n","      count += conf_mat[key][k]\n","    counts[key] = count\n","  return counts\n","\n","print('LENIENT =')\n","counts = get_counts(conf_mat)\n","# {'Drug': 10569, 'Reason': 2519, 'ADE': 604}\n","for key in counts:\n","  print(key, counts[key])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LENIENT =\n","Drug 10569\n","Reason 2519\n","ADE 604\n","O 737415\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UwhyxHJ652EK"},"source":["**Entity F1**"]},{"cell_type":"code","metadata":{"id":"iyWJztmr56EG","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1606771589197,"user_tz":420,"elapsed":184079,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"2ce9aed2-b6f8-47ba-e6aa-eeb54314efd0"},"source":["def get_entity_F1(conf_mat):\n","\n","  stats = {\"Drug\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"P\": 0, \"R\": 0, \"F1\": 0}, \n","           \"Reason\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"P\": 0, \"R\": 0, \"F1\": 0}, \n","           \"ADE\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"P\": 0, \"R\": 0, \"F1\": 0}}\n","  for key in stats:\n","    stats[key][\"TP\"] = conf_mat[key][key]\n","    for k in conf_mat[key]:\n","      if k != key:\n","        stats[key][\"FN\"] += conf_mat[key][k]\n","    for k in conf_mat:\n","      if k != key:\n","        stats[key][\"FP\"] += conf_mat[k][key]\n","        for k2 in conf_mat[k]:\n","          if k2 != key:\n","            stats[key][\"TN\"] += conf_mat[k][k2]\n","    stats[key][\"P\"] = stats[key][\"TP\"] / (stats[key][\"TP\"] + stats[key][\"FP\"])\n","    stats[key][\"R\"] = stats[key][\"TP\"] / (stats[key][\"TP\"] + stats[key][\"FN\"])\n","    stats[key][\"F1\"] = (2 * stats[key][\"P\"] * stats[key][\"R\"]) / (stats[key][\"P\"] + stats[key][\"R\"])\n","  return stats\n","\n","scores = get_entity_F1(conf_mat)\n","scoresDf = pd.DataFrame.from_dict(scores, orient='index')\n","scoresDf.to_csv(SCORES_PATH, sep=\",\", encoding=\"utf-8\")\n","# SOTA= Drug- 0.954 (P- 0.956, R- 0.952), Reason- 0.676 (P- 0.757, R- 0.611), ADE- 0.462 (P- 0.649, R- 0.358)\n","print('LENIENT =')\n","scoresDf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LENIENT =\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TP</th>\n","      <th>TN</th>\n","      <th>FP</th>\n","      <th>FN</th>\n","      <th>P</th>\n","      <th>R</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Drug</th>\n","      <td>10155</td>\n","      <td>740044</td>\n","      <td>494</td>\n","      <td>414</td>\n","      <td>0.953611</td>\n","      <td>0.960829</td>\n","      <td>0.957206</td>\n","    </tr>\n","    <tr>\n","      <th>Reason</th>\n","      <td>1732</td>\n","      <td>748061</td>\n","      <td>527</td>\n","      <td>787</td>\n","      <td>0.766711</td>\n","      <td>0.687574</td>\n","      <td>0.724990</td>\n","    </tr>\n","    <tr>\n","      <th>ADE</th>\n","      <td>288</td>\n","      <td>750337</td>\n","      <td>166</td>\n","      <td>316</td>\n","      <td>0.634361</td>\n","      <td>0.476821</td>\n","      <td>0.544423</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           TP      TN   FP   FN         P         R        F1\n","Drug    10155  740044  494  414  0.953611  0.960829  0.957206\n","Reason   1732  748061  527  787  0.766711  0.687574  0.724990\n","ADE       288  750337  166  316  0.634361  0.476821  0.544423"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"h1I2tAWdQuNq"},"source":["**Micro, Macro, Weighted-average F1(lenient)**"]},{"cell_type":"code","metadata":{"id":"gu75k6sbGQkj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771589493,"user_tz":420,"elapsed":184369,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"674464c5-4f4b-46e7-900b-b5d47afbc1ef"},"source":["def get_micro_f1(scores):\n","  stats = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n","  for key in scores:\n","    for k in scores[key]:\n","      if k in stats:\n","        stats[k] += scores[key][k]\n","  P = stats[\"TP\"] / (stats[\"TP\"] + stats[\"FN\"])\n","  R = stats[\"TP\"] / (stats[\"TP\"] + stats[\"FP\"])\n","  f1 = (2 * P * R) / (P + R)\n","  return f1\n","\n","def get_macro_f1(scores):\n","  f1 = 0\n","  for key in scores:\n","    f1 += scores[key][\"F1\"]\n","  f1 /= 3\n","  return f1\n","\n","def get_weighted_f1(scores, counts):\n","  f1 = 0\n","  totalEntities = 0\n","  for key in scores:\n","    f1 += scores[key][\"F1\"] * counts[key]\n","    totalEntities += counts[key]\n","  f1 /= totalEntities\n","  return f1\n","\n","print('LENIENT =')\n","print('Micro F1- ', get_micro_f1(scores))\n","print('Macro F1- ', get_macro_f1(scores))\n","print('Weighted F1- ', get_weighted_f1(scores, counts))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LENIENT =\n","Micro F1-  0.9000517483551416\n","Macro F1-  0.7422063738498205\n","Weighted F1-  0.8962746239996473\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UyMw_i5iI1YE"},"source":["**Confidence Plot**"]},{"cell_type":"code","metadata":{"id":"x-pLZI9wMGTq"},"source":["# confidence = pd.DataFrame()\n","# confidence['input'] = y_text_total\n","# confidence['actual'] = y_true_total\n","# confidence['pred'] = y_pred_total\n","\n","# confidence['correct'] = np.where((confidence['actual'] == confidence['pred']), y_confidence_total , np.NaN)\n","# confidence['wrong'] = np.where((confidence['actual'] != confidence['pred']), y_confidence_total, np.NaN)\n","# confidence['wrong_actual'] = np.where((confidence['actual'] != confidence['pred']), confidence['actual'], np.NaN)\n","# confidence['wrong_pred'] = np.where((confidence['actual'] != confidence['pred']), confidence['pred'], np.NaN)\n","\n","# confidence['correctADE'] = np.where((confidence['actual'] == confidence['pred']) & (confidence['actual']=='ADE') , y_confidence_total , np.NaN)\n","# confidence['wrongADE'] = np.where((confidence['actual'] != confidence['pred']) & (confidence['actual']=='ADE'), y_confidence_total, np.NaN)\n","\n","# confidence['correctReason'] = np.where((confidence['actual'] == confidence['pred']) & (confidence['actual']=='Reason') , y_confidence_total , np.NaN)\n","# confidence['wrongReason'] = np.where((confidence['actual'] != confidence['pred']) & (confidence['actual']=='Reason'), y_confidence_total, np.NaN)\n","\n","# confidence['correctDrug'] = np.where((confidence['actual'] == confidence['pred']) & (confidence['actual']=='Drug') , y_confidence_total , np.NaN)\n","# confidence['wrongDrug'] = np.where((confidence['actual'] != confidence['pred']) & (confidence['actual']=='Drug'), y_confidence_total, np.NaN)\n","\n","# CONFIDENCE_PATH = OUTPUT_DIR + \"/confidence.csv\"\n","# confidence.to_csv(CONFIDENCE_PATH, columns = ['input', 'wrong_actual', 'wrong_pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkKk-v-tNB4X"},"source":["# confidence.plot(y=['wrong'], style=['ro'], figsize=(72,48))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MARA08omab0D"},"source":["# confidence.plot(y=['correct'], style=['go'], figsize=(72,48))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTa3J08X09Kr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606771589914,"user_tz":420,"elapsed":184773,"user":{"displayName":"Soujanya r. bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64","userId":"04401355901759646794"}},"outputId":"8e0ebba6-e240-44e5-a0ab-1a6b7adb7dd7"},"source":["!ls '$OUTPUT_DIR'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["confusion_matrix.csv  predictions.csv  result.txt  strict_confusion_matrix.csv\n","loss.png\t      result.gdoc      scores.csv\n"],"name":"stdout"}]}]}