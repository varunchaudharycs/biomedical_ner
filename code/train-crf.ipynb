{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train-crf.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb06wqbWDz5T"
   },
   "source": [
    "**Train the BioNER model on N2C2 2018 Track 2 dataset using Clinical-BERT. Save to /model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-XGajYbvv3l"
   },
   "source": [
    "**Final training notebook for BERT-CRF model**\n",
    "\n",
    "\n",
    "**Data versions**\n",
    "- v2 = reduced max seq length to ~100\n",
    "\n",
    "**Model versions**\n",
    "- v5 = train v4 model params + CRF layer\n",
    "- v6 = v5 but 384 max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QTI2X-1-DxOC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791762660,
     "user_tz": 420,
     "elapsed": 669,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpHC6woGD0u7"
   },
   "source": [
    "# Initialize Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLGQEg6lDsBn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791762891,
     "user_tz": 420,
     "elapsed": 891,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "5bfe1183-868b-49db-a4c0-2b629b02fa2c"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wd3An5XTZfK2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791763127,
     "user_tz": 420,
     "elapsed": 1119,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "9dc038f8-726c-4b2f-e6f5-5fb45df6fa8b"
   },
   "source": [
    "!ls '/content/gdrive/My Drive/projects/biomedical_ner/model'"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "v4  v5\tv6  v7\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZJr_sdXurZ2N",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791763128,
     "user_tz": 420,
     "elapsed": 1113,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "DATA_VER = \"v2\"\n",
    "MODEL_VER = \"v5\"\n",
    "PARENT_DIR = \"/content/gdrive/My Drive/projects/biomedical_ner\"\n",
    "DATA_DIR = PARENT_DIR + \"/data/\" + DATA_VER\n",
    "MODEL_DIR = PARENT_DIR + \"/model/\" + MODEL_VER\n",
    "TRAIN_DIR = DATA_DIR + \"/train\"\n",
    "VAL_DIR = DATA_DIR + \"/val\"\n",
    "OUTPUT_DIR = PARENT_DIR + \"/output/\" + MODEL_VER\n",
    "\n",
    "MODEL_PATH = MODEL_DIR + \"/pytorch_model.pt\"\n",
    "CONFIG_PATH = MODEL_DIR + \"/config.json\"\n",
    "VOCAB_PATH = MODEL_DIR + \"/vocab.txt\"\n",
    "BERT_VARIANT = \"emilyalsentzer/Bio_Discharge_Summary_BERT\""
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8tmFO9OFobsa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791763129,
     "user_tz": 420,
     "elapsed": 1110,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "import os\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "  os.makedirs(MODEL_DIR)\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "  os.makedirs(OUTPUT_DIR)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k__ruTr0uGVQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791763129,
     "user_tz": 420,
     "elapsed": 1107,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "batch_size = 16\n",
    "max_len = 272 # try 384\n",
    "epochs = 30\n",
    "lr = 3e-5\n",
    "pad_label = \"X\"\n",
    "max_grad_norm = 1.0\n",
    "full_finetuning = True\n",
    "dropout = 0.1"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNqHhDGuFEPS"
   },
   "source": [
    "# Requirements Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jxRJvt1EpOq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791768264,
     "user_tz": 420,
     "elapsed": 6235,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "4d62815e-e0a4-46fb-86aa-d45bbeb8e70d"
   },
   "source": [
    "!pip install seqeval\n",
    "!pip install transformers"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EkOFiy4KFSUw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791770850,
     "user_tz": 420,
     "elapsed": 8815,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, SequentialSampler, Dataset, ConcatDataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, BertTokenizer, BertForTokenClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "# from pytorch_pretrained_bert.optimization import BertAdam"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QUe5m3JFdS5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771537,
     "user_tz": 420,
     "elapsed": 9497,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "fe1d13c2-15b0-4aee-ab8f-726a9f1484af"
   },
   "source": [
    "# Check library version\n",
    "!pip list | grep -E 'transformers|torch|Keras'"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Keras                         2.4.3          \n",
      "Keras-Preprocessing           1.1.2          \n",
      "torch                         1.7.0+cu101    \n",
      "torchsummary                  1.5.1          \n",
      "torchtext                     0.3.1          \n",
      "torchvision                   0.8.1+cu101    \n",
      "transformers                  4.0.0          \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLvXMrbD2dt9"
   },
   "source": [
    "# Setup Mapping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gec00YIoxhYx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771538,
     "user_tz": 420,
     "elapsed": 9491,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "tag2idx = {'B-Drug': 0,\n",
    "          'I-Drug': 1,\n",
    "          'B-Reason': 2,\n",
    "          'I-Reason': 3,\n",
    "          'B-ADE': 4,\n",
    "          'I-ADE': 5,\n",
    "          'O': 6,\n",
    "          'X': 7,\n",
    "          '[CLS]': 8,\n",
    "          '[SEP]': 9\n",
    "          }\n",
    "tag2name = {tag2idx[key] : key for key in tag2idx}\n",
    "# class_weights = torch.tensor([5.667039548812603, 30.35792759051186, 24.878964599959076, 28.26208740120874, 99.69946699466995, 116.96344396344396, 0.11770158405624111, 0, 9.980995772277634, 9.980995772277634])"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e0wETSz0FrW"
   },
   "source": [
    "# Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77XrTxBy0Hv4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771539,
     "user_tz": 420,
     "elapsed": 9486,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "31352d0d-e1b7-4eb6-87b9-2f925b720f89"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGgXt6LwHIhf"
   },
   "source": [
    "# Prepare Data- Load, Concatenate, Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwbXC16t5NEX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771539,
     "user_tz": 420,
     "elapsed": 9478,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "5854fb1b-9c52-4c0e-a07a-e437dada0ed3"
   },
   "source": [
    "!ls '$TRAIN_DIR' | wc -l"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "265\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPuNTMD3HHBa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771932,
     "user_tz": 420,
     "elapsed": 9864,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "0980d74c-1c39-4d84-a7c5-a651a10adc23"
   },
   "source": [
    "!ls '$VAL_DIR' | wc -l"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "38\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P8xS3mFM-7SZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791771933,
     "user_tz": 420,
     "elapsed": 9859,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "class ClinicalDataset(Dataset):\n",
    "    def __init__(self, file, path, max_seq_len, tag2idx, tokenizer):\n",
    "        self.max_seq_len = max_seq_len;\n",
    "        self.path = os.path.join(path, file)\n",
    "        self.df = pd.read_csv(self.path, names=['patientID', 'sentenceID', 'token', 'tag'], keep_default_na=False)\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tokenizer = tokenizer\n",
    "        # Convert Tokens to indices\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        sentences, labels = self.get_sentences(self.df)\n",
    "        tokenized_texts, word_piece_labels = self.tokenize_text(sentences, labels)\n",
    "        # print(tokenized_texts)\n",
    "        # print(word_piece_labels)\n",
    "\n",
    "        # Make text token into id\n",
    "        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                                  maxlen=self.max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        # Make label into id, pad with \"X\" meaning others/wrong\n",
    "        tags = pad_sequences([[tag2idx[l] for l in lab] for lab in word_piece_labels],\n",
    "                             maxlen=self.max_seq_len, value=self.tag2idx[pad_label],\n",
    "                             padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "        # For fine tune of predict, with token mask is 1,pad token is 0\n",
    "        attention_masks = [[int(i > 0) for i in ii] for ii in input_ids]\n",
    "\n",
    "        self.Sentences = torch.tensor(input_ids)\n",
    "        self.label_data = torch.tensor(tags)\n",
    "        self.attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    def get_sentences(self, data):\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"token\"].values.tolist(), s[\"tag\"].values.tolist())]\n",
    "        grouped = data.groupby(\"sentenceID\").apply(agg_func)\n",
    "        tokenstags = [s for s in grouped]\n",
    "        sentences = [[s[0] for s in sent] for sent in tokenstags]\n",
    "        labels = [[s[1] for s in sent] for sent in tokenstags]\n",
    "        return sentences, labels\n",
    "\n",
    "    def tokenize_text(self, sentences, labels):\n",
    "        tokenized_texts = []\n",
    "        word_piece_labels = []\n",
    "        i_inc = 0\n",
    "        for word_list, label in (zip(sentences,labels)):\n",
    "            temp_label = []\n",
    "            temp_token = []\n",
    "\n",
    "            # Add [CLS] at the front\n",
    "            temp_label.append('[CLS]')\n",
    "            temp_token.append('[CLS]')\n",
    "\n",
    "            for word,lab in zip(word_list,label):\n",
    "                token_list = self.tokenizer.tokenize(word)\n",
    "                for m,token in enumerate(token_list):\n",
    "                    temp_token.append(token)\n",
    "                    if lab.startswith('B'):\n",
    "                        if m==0:\n",
    "                            temp_label.append(lab)\n",
    "                        else:\n",
    "                            temp_label.append('I-'+lab.split('-')[1])\n",
    "                    else:\n",
    "                        temp_label.append(lab)\n",
    "\n",
    "            # Add [SEP] at the end\n",
    "            temp_token.append('[SEP]')\n",
    "            temp_label.append('[SEP]')\n",
    "\n",
    "            tokenized_texts.append(temp_token)\n",
    "            word_piece_labels.append(temp_label)\n",
    "\n",
    "        return tokenized_texts, word_piece_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Sentences[idx], self.attention_masks[idx], self.label_data[idx]"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fNJyqP1Yeujw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791773719,
     "user_tz": 420,
     "elapsed": 11641,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_VARIANT)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aKFz1RF3IE0X",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791890658,
     "user_tz": 420,
     "elapsed": 128576,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# TRAIN DATASET\n",
    "train_datasets = []\n",
    "\n",
    "for doc in os.listdir(TRAIN_DIR):\n",
    "    train_datasets.append(ClinicalDataset(doc, TRAIN_DIR, max_len, tag2idx, tokenizer))\n",
    "\n",
    "# concatenate CSV data\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "\n",
    "train_sampler = SequentialSampler(train_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size) # drop_last=True"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SQq_3z_rrhE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791890661,
     "user_tz": 420,
     "elapsed": 128573,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    },
    "outputId": "a3463238-26bf-48ed-e9cc-e18ddc056ec3"
   },
   "source": [
    "print(f'Dataset length - {len(train_dataset)}, Dataloader length - {len(train_dataloader)}')"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Dataset length - 8121, Dataloader length - 508\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4VuzzPCwIzEb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791905778,
     "user_tz": 420,
     "elapsed": 143685,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# VAL DATASET\n",
    "val_datasets = []\n",
    "\n",
    "for doc in os.listdir(VAL_DIR):\n",
    "    val_datasets.append(ClinicalDataset(doc, VAL_DIR, max_len, tag2idx, tokenizer))\n",
    "\n",
    "# concatenate CSV data\n",
    "val_dataset = ConcatDataset(val_datasets)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUm_xsEIYvs_"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7BQvVzx9fBey",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791905780,
     "user_tz": 420,
     "elapsed": 143680,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# torch.cuda.empty_cache()"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8af9GvP3G21k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791909156,
     "user_tz": 420,
     "elapsed": 147049,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n",
    "\n",
    "\n",
    "class BERT_CRF_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(BERT_CRF_NER, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel \n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)): \n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "        \n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        bert_seq_out, last_hidden = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask)\n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        ''' \n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    # this forward is just for predict, not for train\n",
    "    # dont confuse this with _forward_alg above.\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        gold_score = self._score_sentence(bert_feats, label_seq_ids)\n",
    "\n",
    "        return torch.mean(gold_score - score), label_seq_ids\n",
    "\n",
    "\n",
    "#bert_model = BertModel.from_pretrained(bert_model_scale)\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(BERT_VARIANT, output_hidden_states=False, return_dict = False)\n",
    "start_label_id = tag2idx[\"[CLS]\"]\n",
    "stop_label_id = tag2idx[\"[SEP]\"]\n",
    "\n",
    "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(tag2idx), max_len, batch_size, device)\n"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xqbT8rQnZzBV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791914057,
     "user_tz": 420,
     "elapsed": 151945,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "model.cuda();\n",
    "# loss_weights = torch.FloatTensor(class_weights).cuda()"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kz65d4zNL3SD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791914059,
     "user_tz": 420,
     "elapsed": 151943,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "weight_decay_finetune = 1e-5\n",
    "weight_decay_crf_fc = 5e-6\n",
    "lr0_crf_fc = 8e-5\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
    "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "\n",
    "# def warmup_linear(x, warmup=0.002):\n",
    "#     if x < warmup:\n",
    "#         return x/warmup\n",
    "#     return 1.0 - x"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-I3Ca8qFZ5gl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791914060,
     "user_tz": 420,
     "elapsed": 151941,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# if full_finetuning:\n",
    "#     # Fine tune model all layer parameters\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = ['bias', 'gamma', 'beta']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#          'weight_decay_rate': 0.01},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#          'weight_decay_rate': 0.0}\n",
    "#     ]\n",
    "# else:\n",
    "#     # Only fine tune classifier parameters\n",
    "#     param_optimizer = list(model.classifier.named_parameters()) \n",
    "#     optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8) # (default=1e-6)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xOWiowBA9zQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1606791914060,
     "user_tz": 420,
     "elapsed": 151938,
     "user": {
      "displayName": "Soujanya r. bhat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaasb6g4-yJ0Fcg_hCO8Cl0lDq0AU20eox3yJ6ag=s64",
      "userId": "04401355901759646794"
     }
    }
   },
   "source": [
    "# Scheduler\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KzicQ6jAaMpZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ab7c190-6c1c-414a-e674-8dbcfea93747"
   },
   "source": [
    "print(\"\\n***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(train_dataset)))\n",
    "print(\"  Batch size = %d\"%(batch_size))\n",
    "loss_values, val_loss_values = [], []\n",
    "best_f1 = float(\"-inf\")\n",
    "invalid_tags = set([\"X\", \"[CLS]\", \"[SEP]\"])\n",
    "for epoch in trange(epochs,desc=\"Epoch\"):\n",
    "    model.train();\n",
    "    tr_loss = 0\n",
    "    # nb_tr_examples = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model.neg_log_likelihood(b_input_ids, None, b_input_mask, b_labels)\n",
    "\n",
    "        # forward pass\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, \n",
    "        #                 attention_mask=b_input_mask, labels = b_labels)\n",
    "        # loss = outputs[0]\n",
    "        \n",
    "\n",
    "        # Custom loss calculation\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels = None)\n",
    "        # logits = outputs[0]\n",
    "        # loss = None\n",
    "        # attention_mask = b_input_mask\n",
    "        # labels = b_labels\n",
    "\n",
    "        # loss_fct = CrossEntropyLoss(weight=loss_weights)\n",
    "        # if attention_mask is not None:\n",
    "        #     active_loss = attention_mask.view(-1) == 1\n",
    "        #     active_logits = logits.view(-1, len(tag2idx))\n",
    "        #     active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "        #     loss = loss_fct(active_logits, active_labels)\n",
    "        # else:\n",
    "        #     loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        # nb_tr_examples += b_input_ids.size(0)\n",
    "        \n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    # print and store train loss\n",
    "    train_loss = (tr_loss / len(train_dataset))\n",
    "    loss_values.append(train_loss)\n",
    "    print(\"Train loss: {}\".format(train_loss))\n",
    "\n",
    "    # VALIDATION STEP\n",
    "    model.eval();\n",
    "    val_loss = 0\n",
    "    # nb_eval_examples = 0\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          # forward pass\n",
    "          # outputs = model(b_input_ids, token_type_ids=None,\n",
    "          # attention_mask=b_input_mask, labels=b_labels)\n",
    "          _, predicted_label_seq_ids = model(b_input_ids, None, b_input_mask)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        # logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predicted_label_ids = predicted_label_seq_ids.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the loss for this batch of test sentences.\n",
    "        val_loss += loss.item()\n",
    "        # nb_eval_examples += b_input_ids.size(0)\n",
    "        # predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        predictions.extend(predicted_label_ids)\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = (val_loss / len(val_dataset))\n",
    "    val_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "\n",
    "    # pred_tags = [[tag2name[p_i] for p_i, l_i in zip(p, l) if (tag2name[l_i] != \"X\" and tag2name[l_i] != \"[CLS]\" and tag2name[l_i] != \"[SEP]\")] for p, l in zip(predictions, true_labels)]\n",
    "    pred_tags = [[tag2name[p_i] for p_i, l_i in zip(p, l) if tag2name[l_i] not in invalid_tags] for p, l in zip(predictions, true_labels)]\n",
    "    # valid_tags = [[tag2name[l_i] for l_i in l if (tag2name[l_i] != \"X\" and tag2name[l_i] != \"[CLS]\" and tag2name[l_i] != \"[SEP]\")] for l in true_labels]\n",
    "    valid_tags = [[tag2name[l_i] for l_i in l if tag2name[l_i] not in invalid_tags] for l in true_labels]\n",
    "\n",
    "    report = classification_report(valid_tags, pred_tags,digits=4)\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    f1 = f1_score(valid_tags, pred_tags)\n",
    "    print(\"F1 score: %f\"%(f1))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(valid_tags, pred_tags)))\n",
    "\n",
    "    # SAVE MODEL\n",
    "    if f1 > best_f1:\n",
    "      best_f1 = f1\n",
    "      print('Saving model for BEST f1 - ', best_f1)\n",
    "      torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_f1': best_f1}, MODEL_PATH)\n",
    "      tokenizer.save_vocabulary(MODEL_DIR)\n",
    "      # savemodel = model.module if hasattr(model, 'module') else model\n",
    "      # torch.save(savemodel.state_dict(), MODEL_PATH)\n",
    "      # savemodel.config.to_json_file(CONFIG_PATH)\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running training *****\n",
      "  Num examples = 8121\n",
      "  Batch size = 16\n",
      "Train loss: 613.3536532528514\n",
      "Validation loss: 629.2907700614072\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.1778    0.0777    0.1081       103\n",
      "        Drug     0.9008    0.9070    0.9039      2043\n",
      "      Reason     0.5027    0.6286    0.5586       447\n",
      "\n",
      "   micro avg     0.8050    0.8261    0.8154      2593\n",
      "   macro avg     0.5271    0.5378    0.5236      2593\n",
      "weighted avg     0.8035    0.8261    0.8128      2593\n",
      "\n",
      "F1 score: 0.815379\n",
      "Accuracy score: 0.984810\n",
      "Saving model for BEST f1 -  0.8153787590407309\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:   3%|▎         | 1/30 [09:51<4:45:45, 591.21s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 610.2353538472787\n",
      "Validation loss: 626.441165565154\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3690    0.3010    0.3316       103\n",
      "        Drug     0.9145    0.9055    0.9100      2043\n",
      "      Reason     0.5801    0.6398    0.6085       447\n",
      "\n",
      "   micro avg     0.8335    0.8357    0.8346      2593\n",
      "   macro avg     0.6212    0.6154    0.6167      2593\n",
      "weighted avg     0.8352    0.8357    0.8350      2593\n",
      "\n",
      "F1 score: 0.834585\n",
      "Accuracy score: 0.986014\n",
      "Saving model for BEST f1 -  0.8345850182938571\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:   7%|▋         | 2/30 [19:56<4:37:54, 595.51s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 607.2534104530307\n",
      "Validation loss: 623.2900512593671\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3265    0.3107    0.3184       103\n",
      "        Drug     0.8936    0.9212    0.9072      2043\n",
      "      Reason     0.4778    0.6734    0.5590       447\n",
      "\n",
      "   micro avg     0.7816    0.8542    0.8163      2593\n",
      "   macro avg     0.5660    0.6351    0.5949      2593\n",
      "weighted avg     0.7994    0.8542    0.8238      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  10%|█         | 3/30 [29:55<4:28:26, 596.54s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.816289\n",
      "Accuracy score: 0.983320\n",
      "\n",
      "Train loss: 604.0196493971071\n",
      "Validation loss: 619.950454699209\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3217    0.3592    0.3394       103\n",
      "        Drug     0.8842    0.9232    0.9033      2043\n",
      "      Reason     0.5050    0.6801    0.5796       447\n",
      "\n",
      "   micro avg     0.7814    0.8589    0.8183      2593\n",
      "   macro avg     0.5703    0.6542    0.6074      2593\n",
      "weighted avg     0.7965    0.8589    0.8251      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  13%|█▎        | 4/30 [39:55<4:18:53, 597.44s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.818299\n",
      "Accuracy score: 0.983871\n",
      "\n",
      "Train loss: 600.5668678354151\n",
      "Validation loss: 616.2025623178602\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3468    0.4175    0.3789       103\n",
      "        Drug     0.8879    0.9232    0.9052      2043\n",
      "      Reason     0.5320    0.6689    0.5927       447\n",
      "\n",
      "   micro avg     0.7929    0.8592    0.8247      2593\n",
      "   macro avg     0.5889    0.6698    0.6256      2593\n",
      "weighted avg     0.8051    0.8592    0.8304      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  17%|█▋        | 5/30 [49:56<4:09:24, 598.59s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.824727\n",
      "Accuracy score: 0.984324\n",
      "\n",
      "Train loss: 596.9208279099903\n",
      "Validation loss: 612.4746923136969\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3577    0.4272    0.3894       103\n",
      "        Drug     0.9032    0.9139    0.9085      2043\n",
      "      Reason     0.5731    0.6398    0.6047       447\n",
      "\n",
      "   micro avg     0.8170    0.8473    0.8319      2593\n",
      "   macro avg     0.6114    0.6603    0.6342      2593\n",
      "weighted avg     0.8247    0.8473    0.8355      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  20%|██        | 6/30 [59:57<3:59:44, 599.35s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.831882\n",
      "Accuracy score: 0.985438\n",
      "\n",
      "Train loss: 593.0654325735362\n",
      "Validation loss: 608.437340627602\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3800    0.3689    0.3744       103\n",
      "        Drug     0.9074    0.9109    0.9091      2043\n",
      "      Reason     0.6096    0.6219    0.6157       447\n",
      "\n",
      "   micro avg     0.8351    0.8396    0.8373      2593\n",
      "   macro avg     0.6323    0.6339    0.6331      2593\n",
      "weighted avg     0.8351    0.8396    0.8373      2593\n",
      "\n",
      "F1 score: 0.837308\n",
      "Accuracy score: 0.986597\n",
      "Saving model for BEST f1 -  0.8373076923076924\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  23%|██▎       | 7/30 [1:10:00<3:50:11, 600.50s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 589.0425648853089\n",
      "Validation loss: 604.0624512125312\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.4144    0.4466    0.4299       103\n",
      "        Drug     0.8965    0.9119    0.9041      2043\n",
      "      Reason     0.6164    0.6577    0.6364       447\n",
      "\n",
      "   micro avg     0.8263    0.8496    0.8378      2593\n",
      "   macro avg     0.6424    0.6721    0.6568      2593\n",
      "weighted avg     0.8291    0.8496    0.8391      2593\n",
      "\n",
      "F1 score: 0.837802\n",
      "Accuracy score: 0.986072\n",
      "Saving model for BEST f1 -  0.8378018634721429\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  27%|██▋       | 8/30 [1:20:06<3:40:45, 602.07s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 584.7956064668375\n",
      "Validation loss: 599.764190648418\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.4184    0.3981    0.4080       103\n",
      "        Drug     0.9052    0.9158    0.9105      2043\n",
      "      Reason     0.6211    0.6309    0.6260       447\n",
      "\n",
      "   micro avg     0.8377    0.8461    0.8419      2593\n",
      "   macro avg     0.6482    0.6482    0.6481      2593\n",
      "weighted avg     0.8369    0.8461    0.8415      2593\n",
      "\n",
      "F1 score: 0.841903\n",
      "Accuracy score: 0.986500\n",
      "Saving model for BEST f1 -  0.841903300076746\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  30%|███       | 9/30 [1:30:09<3:30:46, 602.20s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 580.3393622629602\n",
      "Validation loss: 595.0070221430059\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.4087    0.4563    0.4312       103\n",
      "        Drug     0.8991    0.9202    0.9095      2043\n",
      "      Reason     0.5912    0.6600    0.6237       447\n",
      "\n",
      "   micro avg     0.8214    0.8569    0.8388      2593\n",
      "   macro avg     0.6330    0.6788    0.6548      2593\n",
      "weighted avg     0.8265    0.8569    0.8413      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  33%|███▎      | 10/30 [1:40:10<3:20:42, 602.11s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.838807\n",
      "Accuracy score: 0.985956\n",
      "\n",
      "Train loss: 575.7025233817033\n",
      "Validation loss: 589.9684280027061\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.4375    0.4078    0.4221       103\n",
      "        Drug     0.8940    0.9202    0.9069      2043\n",
      "      Reason     0.6190    0.6398    0.6293       447\n",
      "\n",
      "   micro avg     0.8298    0.8515    0.8405      2593\n",
      "   macro avg     0.6502    0.6559    0.6528      2593\n",
      "weighted avg     0.8284    0.8515    0.8398      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  37%|███▋      | 11/30 [1:50:10<3:10:25, 601.34s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.840502\n",
      "Accuracy score: 0.986390\n",
      "\n",
      "Train loss: 570.9192069797823\n",
      "Validation loss: 584.6872170326811\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3984    0.4757    0.4336       103\n",
      "        Drug     0.8952    0.9280    0.9113      2043\n",
      "      Reason     0.5992    0.6555    0.6261       447\n",
      "\n",
      "   micro avg     0.8198    0.8631    0.8409      2593\n",
      "   macro avg     0.6309    0.6864    0.6570      2593\n",
      "weighted avg     0.8244    0.8631    0.8432      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  40%|████      | 12/30 [2:00:08<3:00:04, 600.27s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.840879\n",
      "Accuracy score: 0.986312\n",
      "\n",
      "Train loss: 565.9539300237233\n",
      "Validation loss: 580.0818230901332\n",
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADE     0.3945    0.4175    0.4057       103\n",
      "        Drug     0.9082    0.9099    0.9090      2043\n",
      "      Reason     0.6188    0.6644    0.6408       447\n",
      "\n",
      "   micro avg     0.8342    0.8481    0.8411      2593\n",
      "   macro avg     0.6405    0.6639    0.6518      2593\n",
      "weighted avg     0.8379    0.8481    0.8428      2593\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\rEpoch:  43%|████▎     | 13/30 [2:10:08<2:50:05, 600.35s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "F1 score: 0.841079\n",
      "Accuracy score: 0.986111\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uNLi9jSCECzv"
   },
   "source": [
    "# print('Saving model for BEST loss - ', best_val_loss)\n",
    "# savemodel = model.module if hasattr(model, 'module') else model\n",
    "# torch.save(savemodel.state_dict(), MODEL_PATH)\n",
    "# savemodel.config.to_json_file(CONFIG_PATH)\n",
    "# tokenizer.save_vocabulary(MODEL_DIR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5w03pomr21Xc"
   },
   "source": [
    "!ls '$MODEL_DIR'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drERJcpqF4O2"
   },
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pGTkbHYrFPIj"
   },
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(val_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(OUTPUT_DIR + \"/loss.png\")\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zWlQEulT35ch"
   },
   "source": [
    "!ls '$OUTPUT_DIR'"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}